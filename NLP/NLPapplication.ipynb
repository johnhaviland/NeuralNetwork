{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> NLP Application </h1>\n",
    "\n",
    "<h2> by Nathan Dilla & John Haviland </h2>\n",
    "\n",
    "<h3> Problem Statement </h3>\n",
    "\n",
    "\n",
    "<h2> Dataset Overview </h2>\n",
    "<h3> Purpose </h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 1: Import Libraries </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.chunk import ChunkParserI\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "from nltk.metrics import precision, recall, f_measure\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 2: Load in Dataset </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hate_speech_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 3: Download NLTK Data (Optional) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 4: Tokenize, POS Tag Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['OriginalTweet'].apply(word_tokenize)\n",
    "data['pos_tags'] = data['tokens'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 5: Split Annotated Data into Training & Test Sets </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=0.2, random_state=None):\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    return train_data, test_data\n",
    "\n",
    "# Define your annotated data in the format (word, ne_tag)\n",
    "annotated_data = [\n",
    "    [\n",
    "        ('Word1', 'B-ENTITY'),\n",
    "        ('Word2', 'I-ENTITY'),\n",
    "        ('Word3', 'O'),\n",
    "        # More words and tags for the first sentence\n",
    "    ],\n",
    "    [\n",
    "        ('Word1', 'B-ENTITY'),\n",
    "        ('Word2', 'I-ENTITY'),\n",
    "        ('Word3', 'I-ENTITY'),\n",
    "        # More words and tags for the second sentence\n",
    "    ],\n",
    "    # More sentences\n",
    "]\n",
    "\n",
    "# Split your annotated data into training and test sets\n",
    "train_data, test_data = split_data(annotated_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 6: Define NER Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCoronaNER(ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        # Convert the annotated data to the format used in the example\n",
    "        train_data = [([(word, ne_tag) for (word, ne_tag) in sent]) for sent in train_sents]\n",
    "        self.tagger = BigramTagger(train_data, backoff=UnigramTagger(train_data))\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        # Predict chunk tags\n",
    "        tagged_ne_tags = self.tagger.tag(sentence)\n",
    "        conlltags = [(word, 'POS', ne_tag) for (word, ne_tag) in tagged_ne_tags]\n",
    "        # Convert to tree\n",
    "        return conlltags2tree(conlltags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 7: Train & Evaluate NER System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_ner = CustomCoronaNER(train_data)\n",
    "\n",
    "gold = [tree2conlltags(conlltags2tree([(word, 'POS', ne_tag) for word, ne_tag in sent])) for sent in test_data]\n",
    "test = [tree2conlltags(corona_ner.parse(sent)) for sent in test_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 8: Compute Performance Metrics </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_chunks = set(gold[0] + gold[1])\n",
    "test_chunks = set(test[0] + test[1])\n",
    "\n",
    "precision_score = precision(gold_chunks, test_chunks)\n",
    "recall_score = recall(gold_chunks, test_chunks)\n",
    "f_measure_score = f_measure(gold_chunks, test_chunks)\n",
    "\n",
    "print('Precision:', precision_score)\n",
    "print('Recall:', recall_score)\n",
    "print('F-measure:', f_measure_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Analysis of our Findings </h3>\n",
    "\n",
    "\n",
    "\n",
    "<h3> References </h3>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
