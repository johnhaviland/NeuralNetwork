{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Recurrent Neural Network </h1>\n",
    "<h2> by Nathan Dilla & John Haviland </h2>\n",
    "\n",
    "<h3> Problem Statement </h3>\n",
    "\n",
    "<h2> Dataset Overview </h2>\n",
    "\n",
    "<h3> Purpose </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 1: Import Libraries, Load & Preprocess Dataset </h3>\n",
    "\n",
    "In this step, we import in the necessary libraries and load in the text dataset \"poem.txt\" and split it into sentences. We then preprocess each sentence by removing the punctuation and splitting the sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load in 'poem.txt' dataset with UTF-8 encoding\n",
    "with open('poem.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split the text into sentences/lines\n",
    "sentences = text.split('\\n')\n",
    "\n",
    "# Preprocess the lines by removing punctuation and splitting into words\n",
    "sentences = [re.sub(r'[^\\w\\s]', '', sentence).lower().split() for sentence in sentences if sentence.strip() != '']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 2: Tokenize, Prepare Sequences </h3>\n",
    "\n",
    "We use the Tokenizer from the Keras library to convert the words in the lines into integer values. We create sequences and labels using a \"sliding window\" approach, where each sequence is a list of words and the last word in the sequences is the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create sequences and labels using 'sliding window' approach\n",
    "input_sequences = []\n",
    "for line in sentences:\n",
    "    for i in range(1, len(line)):\n",
    "        n_gram_sequence = line[:i + 1]\n",
    "        # Check if all words in the sequence are in the tokenizer's word index\n",
    "        if all(word in tokenizer.word_index for word in n_gram_sequence):\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Check the sequences for any empty lists\n",
    "input_sequences = [seq for seq in input_sequences if len(seq) > 0]\n",
    "\n",
    "# Convert the sequences to integer values using the tokenizer\n",
    "input_sequences = [tokenizer.texts_to_sequences(seq) for seq in input_sequences]\n",
    "\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 3: Load in Pre-trained GloVe Embeddings </h3>\n",
    "\n",
    "In this step, we load in pre-trained GloVe word embeddings from the file 'glove.6B.100d.txt', creating an embedding matrix. The words in the dataset are matched to the pre-trained embeddings, and the matrix is created with the dimensions (total_words, 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((total_words, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 4: Build & Compile LSTM Model </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model w/ embedding and dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, weights=[embedding_matrix], input_length=max_sequence_length - 1, trainable=False))\n",
    "model.add(Masking(mask_value=0.0))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Print LSTM model architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 5: Train the Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks (using ModelCheckpoint and EarlyStopping)\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# Train model\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, verbose=2, callbacks=[checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 6: Test the Model (Generate Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_length):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted)\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "# Test the model by generating text\n",
    "seed_text = \"The best advice is\"\n",
    "generated_text = generate_text(seed_text, next_words=3, model=model, max_sequence_length=max_sequence_length)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 7: Explore Embeddings using Cosine Similarity </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"girl\"\n",
    "word2 = \"boy\"\n",
    "index1 = tokenizer.word_index[word1]\n",
    "index2 = tokenizer.word_index[word2]\n",
    "vector1 = embedding_matrix[index1]\n",
    "vector2 = embedding_matrix[index2]\n",
    "similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
    "print(f\"Cosine Similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "\n",
    "# Create vectors for visualization\n",
    "origin = np.zeros(2)\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(*origin, vector1[0], vector1[1], angles='xy', scale_units='xy', scale=1, color='r', label=word1)\n",
    "ax.quiver(*origin, vector2[0], vector2[1], angles='xy', scale_units='xy', scale=1, color='b', label=word2)\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 8: Compute Performance Metrics </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Analysis of our Findings </h2>\n",
    "\n",
    "\n",
    "\n",
    "<h2> References </h2>\n",
    "\n",
    "https://www.kaggle.com/datasets/harshalgadhe/poem-generation/\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
